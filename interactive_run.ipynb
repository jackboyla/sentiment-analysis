{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "41678a10",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train/Val/Test Splits: 8784/2928/2928\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('./src')\n",
        "import dataflow\n",
        "import model as models\n",
        "import utils\n",
        "\n",
        "import lightning as L\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "cfg_path = 'config.yaml'\n",
        "cfg = OmegaConf.load(cfg_path)\n",
        "\n",
        "dm = dataflow.TweetDataModule(cfg)\n",
        "dm.prepare_data()\n",
        "\n",
        "# splits/transforms\n",
        "dm.setup(stage=\"fit\")\n",
        "\n",
        "cfg.data_processing.label_decode_map = dm.label_decode_map\n",
        "cfg.data_processing.tokenizer.kwargs.vocab_size = dm.tokenizer.vocab_size\n",
        "cfg.hyperparameters.backbone.kwargs.input_size = dm.tokenizer.vocab_size\n",
        "\n",
        "sample = next(iter(dm.train_dataloader()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bdfda51b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-05-20 23:11:53,781 model INFO: Frozen the first 123 out of 246 encoder weights\n"
          ]
        }
      ],
      "source": [
        "model = models.SentimentClassifier.load_from_checkpoint('wandb-sentiment-analysis/0vfrmtju/checkpoints/epoch=4-step=2745.ckpt', tokenizer=dm.tokenizer, hyperparams=cfg.hyperparameters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7fb522ac",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.5837,  0.5044, -1.5180],\n",
              "        [ 2.0489, -0.8427, -2.4948],\n",
              "        [ 2.5640, -1.6338, -2.7393],\n",
              "        [ 0.3040,  0.7500, -1.3418],\n",
              "        [ 0.1069,  0.0412, -0.2656],\n",
              "        [ 2.0290, -0.7391, -2.4948],\n",
              "        [ 1.4048, -0.0654, -2.2017],\n",
              "        [ 1.9650, -0.6771, -2.4681],\n",
              "        [ 0.4626, -0.3696, -0.2962],\n",
              "        [ 3.1427, -1.2991, -3.8980],\n",
              "        [ 5.0072, -3.1753, -5.6953],\n",
              "        [ 0.8798, -2.1245,  0.2674],\n",
              "        [ 0.1739,  1.1039, -1.5528],\n",
              "        [ 2.8586, -1.5739, -3.1581],\n",
              "        [-0.7312,  1.0878, -0.4434],\n",
              "        [ 1.4208, -0.5154, -1.7409]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logits = model.get_logits(sample[0])\n",
        "logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "08d3ac06",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d32e4aec",
      "metadata": {
        "id": "d32e4aec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.argmax(logits, dim=1).flatten().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bea87d0",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
