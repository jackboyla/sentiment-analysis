seed: 42
debug_dataflow: True

datafiles:
    data_dirs: {
        'data/science-challenge-master-data/negative': 'negative',
        'data/science-challenge-master-data/positive': 'positive',
        'data/science-challenge-master-data/neutral': 'neutral'
        }
    kwargs: 
        names: ['text']

data_processing:
    tokenizer:
        object: transformers.CanineTokenizer.from_pretrained
        kwargs: 
            pretrained_model_name_or_path: 'google/canine-c'
            return_tensors: 'pt'
    train_size: 0.6
    val_size: 0.2
    test_size: 0.2

hyperparameters:
    backbone: 
        object: transformer.TransformerEncoderModel
        required_args: {'input_ids', 'src_key_padding_mask'}
        kwargs: 
            hidden_size: 256
            num_layers: 2
            num_heads: 2
            dropout: 0.1
            
            embedding: 
                object: transformers.models.canine.modeling_canine.CanineEmbeddings
            num_hash_functions: 4
            num_hash_buckets: 250 # 16384
            type_vocab_size: 16
            layer_norm_eps: 1e-12
            hidden_dropout_prob: 0.1
            max_position_embeddings: 250 # 16384

    classifier_head:
        num_layers: 1

    batch_size: 64
    pin_memory: True
    freeze_encoder: 0
    num_classes: 3

    optimizer:
        object: torch.optim.AdamW
        lr:
          backbone: 3.0
          head: 3.0

    scheduler:
        # object: transformers.get_linear_schedule_with_warmup
        object: torch.optim.lr_scheduler.ExponentialLR
        kwargs:
            num_warmup_steps: 0
            # step_size: 1.0
            gamma: 0.5
            # eta_min: 1e-5
            interval: 'epoch'
            frequency: 1

    trainer:
        accelerator: 'auto'
        enable_progress_bar: False
        precision: '16-mixed'
        max_epochs: 5
        profiler: 'simple'
        log_every_n_steps: 15

loggers:
    wandb_logger:
        object: lightning.pytorch.loggers.WandbLogger
        kwargs:
            project: 'wandb-sentiment-analysis'
            offline: False

callbacks:
    # early_stopping_callback:
    #     object: lightning.pytorch.callbacks.EarlyStopping
    #     kwargs:
    #         patience: 5
    #         verbose: True
    #         min_delta: 0.05
    #         monitor: 'val_loss'

    learning_rate_monitor: 
        object: lightning.pytorch.callbacks.LearningRateMonitor
        kwargs:
            logging_interval: 'step'

    checkpoint_callback:
        object: lightning.pytorch.callbacks.ModelCheckpoint
        kwargs:
            save_top_k: 2
            monitor: 'val_loss'

    slack_callback: True

    print_table_metrics_callback: True
